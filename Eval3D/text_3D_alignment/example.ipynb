{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, glob, sys, re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input video\n",
    "video_filename = \"a_corgi_taking_a_selfie.mp4\"\n",
    "\n",
    "# pre-generated questions\n",
    "tifa_questions = json.load(open(\"sample_questions.json\"))\n",
    "\n",
    "# working directory to save all the frames and corresponding answers\n",
    "working_folder = \"sample_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract frames from the video\n",
    "from utils import extract_frames\n",
    "\n",
    "frames = extract_frames(video_filename, working_folder,n_div=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQA for all questions\n",
    "# replace get_vqa_answers to use other VLMs\n",
    "from gpt4o import get_vqa_answers\n",
    "\n",
    "def get_vqa_answers_for_dir(image_directory, tifa_questions):\n",
    "    \n",
    "    images = glob.glob(os.path.join(image_directory, \"*.png\"))\n",
    "    \n",
    "    this_prompt_questions = tifa_questions[\"questions\"]\n",
    "    \n",
    "    this_prompt_answers= {\"questions\": this_prompt_questions,\n",
    "                          \"answers\": defaultdict(dict),\n",
    "                          \"scene_graph\": tifa_questions[\"scene_graph\"],\n",
    "                          \"dependencies\": tifa_questions[\"question_dependencies\"]\n",
    "                          }\n",
    "    \n",
    "    for image in images:\n",
    "        answers = get_vqa_answers(image, this_prompt_questions)\n",
    "        this_image = os.path.basename(image)\n",
    "        \n",
    "        for qid, answer in answers.items():\n",
    "            this_prompt_answers[\"answers\"][qid][this_image] = answer\n",
    "            \n",
    "        for qid in this_prompt_questions.keys():\n",
    "            if int(qid) not in answers:\n",
    "                this_prompt_answers[\"answers\"][int(qid)][this_image] = \"N/A\"\n",
    "            \n",
    "    with open(os.path.join(image_directory, \"vqa_answers.json\"), \"w\") as f:\n",
    "        json.dump(this_prompt_answers, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the VQA (may take a while)\n",
    "get_vqa_answers_for_dir(working_folder, tifa_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the alignment score for the model. Also, get the score for each question and record it.\n",
    "\n",
    "def process_directory_tifa(directory, json_name=\"vqa_answers.json\", n_div=12):\n",
    "    tifa_score_dict = json.load(open(os.path.join(directory, json_name)))\n",
    "    per_question_score_dict = {}\n",
    "    \n",
    "    q_ids = list(tifa_score_dict[\"answers\"].keys())\n",
    "    \n",
    "    question_dependencies = tifa_score_dict[\"dependencies\"]\n",
    "    \n",
    "    for q_id in q_ids:\n",
    "        this_scores = []\n",
    "\n",
    "        # first, check dependency\n",
    "        dependency_ids = question_dependencies[q_id]\n",
    "        question_valid_flag = True\n",
    "        for dep_id in dependency_ids:\n",
    "            dep_id = str(dep_id)\n",
    "            if dep_id in per_question_score_dict:\n",
    "                if per_question_score_dict[dep_id] < 0.5:\n",
    "                    question_valid_flag = False\n",
    "                    break\n",
    "        \n",
    "        # if prerequisite question got the wrong answer, then this question is also wrong\n",
    "        if not question_valid_flag:\n",
    "            per_question_score_dict[q_id] = 0\n",
    "            continue\n",
    "        \n",
    "        for i in range(n_div):\n",
    "            try:\n",
    "                answer = tifa_score_dict[\"answers\"][q_id][f\"{i}.png\"]\n",
    "            except:\n",
    "                answer = \"N/A\"\n",
    "            \n",
    "            if \"yes\" in answer.strip().lower():\n",
    "                this_scores.append(1)\n",
    "            else:\n",
    "                this_scores.append(0)\n",
    "        \n",
    "        # now pooling is here\n",
    "        looped_scores = this_scores + this_scores + this_scores\n",
    "        \n",
    "        # if three consecutive yes, then the question is good\n",
    "        for i in range(n_div, 2*n_div):\n",
    "            if looped_scores[i] == 1 and looped_scores[i+1] == 1 and looped_scores[i-1] == 1:\n",
    "                per_question_score_dict[q_id] = 1\n",
    "                break\n",
    "            \n",
    "        if q_id not in per_question_score_dict:\n",
    "            per_question_score_dict[q_id] = 0\n",
    "        \n",
    "    # record per_question_scores\n",
    "    tifa_score_dict[\"alignment_scores_per_question\"] = per_question_score_dict\n",
    "    \n",
    "    # return average score\n",
    "    avg_score =  sum(per_question_score_dict.values())/len(per_question_score_dict)\n",
    "    tifa_score_dict[\"alignment_score\"] = avg_score\n",
    "    \n",
    "    with open(os.path.join(directory, json_name), \"w\") as f:\n",
    "        json.dump(tifa_score_dict, f, indent=2)\n",
    "        \n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the final alignment score\n",
    "\n",
    "process_directory_tifa(working_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvdream",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
